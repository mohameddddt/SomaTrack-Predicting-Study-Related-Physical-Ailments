{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cf0d769",
   "metadata": {},
   "source": [
    "# SomaTrack: Predicting Study-Related Physical Ailments\n",
    "\n",
    "## Project Overview\n",
    "Prolonged sedentary behavior in universities leads to specific physical health issues â€” primarily **back pain**, **neck strain**, and **tension headaches**. SomaTrack is a Machine Learning project that predicts the likelihood of experiencing these physical ailments based on students' study environment and habits.\n",
    "\n",
    "**Problem Type**: Multiclass Classification\n",
    "\n",
    "**Target Labels**:\n",
    "- 0: No Pain\n",
    "- 1: Mild / Occasional Pain  \n",
    "- 2: Moderate / Frequent Pain\n",
    "- 3: Severe / Chronic Pain\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79146b94",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Import Libraries](#1-import-libraries)\n",
    "2. [Load Dataset](#2-load-dataset)\n",
    "3. [Data Overview & Understanding](#3-data-overview--understanding)\n",
    "4. [Data Cleaning](#4-data-cleaning)\n",
    "5. [Feature Engineering](#5-feature-engineering)\n",
    "6. [Exploratory Data Analysis (EDA)](#6-exploratory-data-analysis-eda)\n",
    "7. [Feature Selection & Importance](#7-feature-selection--importance)\n",
    "8. [Data Preprocessing for Modeling](#8-data-preprocessing-for-modeling)\n",
    "9. [Model Development & Comparison](#9-model-development--comparison)\n",
    "10. [Model Evaluation & Validation](#10-model-evaluation--validation)\n",
    "11. [Final Model Selection & Explainability](#11-final-model-selection--explainability)\n",
    "12. [Save Model & Demo](#12-save-model--demo)\n",
    "13. [Conclusions & Recommendations](#13-conclusions--recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866ac002",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ac2b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score,\n",
    "    f1_score, precision_score, recall_score, ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "# Feature Importance\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Model Persistence\n",
    "import joblib\n",
    "\n",
    "# Settings\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print('All libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcab24e9",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1692d9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the survey data\n",
    "# Update the path once you have the CSV from Google Forms\n",
    "df = pd.read_csv('../data/raw/somatrack_survey.csv')\n",
    "\n",
    "# Make a copy to preserve the original\n",
    "df_original = df.copy()\n",
    "\n",
    "print(f'Dataset loaded: {df.shape[0]} rows, {df.shape[1]} columns')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3316b4",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Overview & Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4097495c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic info\n",
    "print('='*60)\n",
    "print('DATASET INFO')\n",
    "print('='*60)\n",
    "df.info()\n",
    "print()\n",
    "\n",
    "print('='*60)\n",
    "print('STATISTICAL SUMMARY (Numerical Features)')\n",
    "print('='*60)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25e8a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print('='*60)\n",
    "print('MISSING VALUES')\n",
    "print('='*60)\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (df.isnull().sum() / len(df)) * 100\n",
    "missing_df = pd.DataFrame({'Missing Count': missing, 'Percentage (%)': missing_pct})\n",
    "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
    "\n",
    "if missing_df.empty:\n",
    "    print('No missing values found!')\n",
    "else:\n",
    "    print(missing_df)\n",
    "\n",
    "print()\n",
    "print('='*60)\n",
    "print('DUPLICATE ROWS')\n",
    "print('='*60)\n",
    "print(f'Number of duplicate rows: {df.duplicated().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b549187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types and unique values for each column\n",
    "print('='*60)\n",
    "print('COLUMN DETAILS')\n",
    "print('='*60)\n",
    "for col in df.columns:\n",
    "    print(f'\\n--- {col} ---')\n",
    "    print(f'  dtype: {df[col].dtype}')\n",
    "    print(f'  unique values: {df[col].nunique()}')\n",
    "    if df[col].nunique() <= 10:\n",
    "        print(f'  value counts:')\n",
    "        print(df[col].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01378daf",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Data Cleaning\n",
    "\n",
    "Tasks:\n",
    "- Handle missing values\n",
    "- Remove duplicates\n",
    "- Fix data types\n",
    "- Rename columns for consistency\n",
    "- Handle outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142efdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RENAME COLUMNS\n",
    "# ============================================================\n",
    "# Rename Google Forms long column names to short, clean names\n",
    "# UPDATE this mapping based on your actual Google Form questions\n",
    "\n",
    "column_mapping = {\n",
    "    # 'Original Google Form Question': 'clean_name'\n",
    "    # Example mappings (update with your actual column names):\n",
    "    # 'What is your age?': 'age',\n",
    "    # 'What is your gender?': 'gender',\n",
    "    # 'How many hours do you spend sitting per day while studying?': 'sitting_hours_per_day',\n",
    "    # 'How many consecutive hours do you study without taking a break?': 'consecutive_hours_no_break',\n",
    "    # 'How many liters of water do you drink per day?': 'water_liters_per_day',\n",
    "    # 'What is your total daily screen time (phone + laptop) in hours?': 'daily_screen_time_hours',\n",
    "    # 'Where do you primarily study?': 'study_location',\n",
    "    # 'What type of seat do you usually use while studying?': 'seat_type',\n",
    "    # 'Do you use external peripherals (mouse/keyboard)?': 'uses_peripherals',\n",
    "    # 'How often do you exercise per week?': 'exercise_frequency',\n",
    "    # 'Do you take regular breaks during study sessions?': 'takes_breaks',\n",
    "    # 'How would you rate your overall posture while studying? (1-10)': 'posture_rating',\n",
    "    # 'Rate your back pain level (0-3)': 'back_pain_level',\n",
    "    # 'Rate your neck pain level (0-3)': 'neck_pain_level',\n",
    "    # 'Rate your headache frequency (0-3)': 'headache_level',\n",
    "    # 'Overall pain level (0-3)': 'pain_level',\n",
    "}\n",
    "\n",
    "# df.rename(columns=column_mapping, inplace=True)  # Uncomment after filling mapping\n",
    "print('Columns after renaming:')\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f328c02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DROP UNNECESSARY COLUMNS\n",
    "# ============================================================\n",
    "# Drop the Google Forms timestamp column and any other irrelevant columns\n",
    "\n",
    "columns_to_drop = ['Timestamp']  # Add any other columns to drop\n",
    "df.drop(columns=[col for col in columns_to_drop if col in df.columns], inplace=True)\n",
    "\n",
    "print(f'Shape after dropping columns: {df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38384b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# HANDLE MISSING VALUES\n",
    "# ============================================================\n",
    "\n",
    "# Strategy:\n",
    "# - Numerical columns: fill with median (robust to outliers)\n",
    "# - Categorical columns: fill with mode (most frequent value)\n",
    "\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f'Numerical columns ({len(numerical_cols)}): {numerical_cols}')\n",
    "print(f'Categorical columns ({len(categorical_cols)}): {categorical_cols}')\n",
    "\n",
    "# Fill missing values\n",
    "for col in numerical_cols:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        df[col].fillna(df[col].median(), inplace=True)\n",
    "        print(f'  Filled {col} with median: {df[col].median()}')\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "        print(f'  Filled {col} with mode: {df[col].mode()[0]}')\n",
    "\n",
    "print(f'\\nRemaining missing values: {df.isnull().sum().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167a5630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# REMOVE DUPLICATES\n",
    "# ============================================================\n",
    "before = len(df)\n",
    "df.drop_duplicates(inplace=True)\n",
    "after = len(df)\n",
    "print(f'Removed {before - after} duplicate rows. Remaining: {after} rows.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4153f8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DETECT & HANDLE OUTLIERS (using IQR method)\n",
    "# ============================================================\n",
    "\n",
    "def detect_outliers_iqr(dataframe, column):\n",
    "    \"\"\"Detect outliers using IQR method.\"\"\"\n",
    "    Q1 = dataframe[column].quantile(0.25)\n",
    "    Q3 = dataframe[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    outliers = dataframe[(dataframe[column] < lower) | (dataframe[column] > upper)]\n",
    "    return outliers, lower, upper\n",
    "\n",
    "print('Outlier Detection (IQR Method):')\n",
    "print('-' * 50)\n",
    "for col in numerical_cols:\n",
    "    outliers, lower, upper = detect_outliers_iqr(df, col)\n",
    "    if len(outliers) > 0:\n",
    "        print(f'{col}: {len(outliers)} outliers (range: {lower:.2f} - {upper:.2f})')\n",
    "\n",
    "# Optional: cap outliers instead of removing them\n",
    "# for col in numerical_cols:\n",
    "#     Q1 = df[col].quantile(0.25)\n",
    "#     Q3 = df[col].quantile(0.75)\n",
    "#     IQR = Q3 - Q1\n",
    "#     df[col] = df[col].clip(Q1 - 1.5 * IQR, Q3 + 1.5 * IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1be49a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned data\n",
    "df.to_csv('../data/processed/somatrack_cleaned.csv', index=False)\n",
    "print(f'Cleaned dataset saved: {df.shape[0]} rows, {df.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef580648",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Feature Engineering\n",
    "\n",
    "Create new features that may improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611994c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CREATE NEW FEATURES\n",
    "# ============================================================\n",
    "\n",
    "# Example engineered features (adjust column names based on your actual data):\n",
    "\n",
    "# 1. Break ratio: how often breaks are taken relative to study time\n",
    "# df['break_ratio'] = df['consecutive_hours_no_break'] / df['sitting_hours_per_day']\n",
    "\n",
    "# 2. Sedentary intensity: screen time * sitting hours\n",
    "# df['sedentary_intensity'] = df['daily_screen_time_hours'] * df['sitting_hours_per_day']\n",
    "\n",
    "# 3. Hydration per sitting hour\n",
    "# df['hydration_per_hour'] = df['water_liters_per_day'] / df['sitting_hours_per_day']\n",
    "\n",
    "# 4. Is the study environment ergonomic? (binary)\n",
    "# df['ergonomic_setup'] = ((df['seat_type'] == 'Ergonomic chair') & \n",
    "#                          (df['uses_peripherals'] == 'Yes')).astype(int)\n",
    "\n",
    "# 5. Risk score: combination of bad habits\n",
    "# df['risk_score'] = (df['sitting_hours_per_day'] * 0.3 + \n",
    "#                     df['consecutive_hours_no_break'] * 0.3 + \n",
    "#                     df['daily_screen_time_hours'] * 0.2 - \n",
    "#                     df['water_liters_per_day'] * 0.1 - \n",
    "#                     df['exercise_frequency'] * 0.1)\n",
    "\n",
    "print('Feature engineering complete.')\n",
    "print(f'Dataset shape: {df.shape}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05a125b",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Comprehensive visualizations to understand our data and campus ergonomic health."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915a11bc",
   "metadata": {},
   "source": [
    "### 6.1 Target Variable Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce212b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TARGET VARIABLE DISTRIBUTION\n",
    "# ============================================================\n",
    "# UPDATE 'pain_level' to your actual target column name\n",
    "\n",
    "TARGET = 'pain_level'  # <-- UPDATE THIS\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Count plot\n",
    "pain_counts = df[TARGET].value_counts().sort_index()\n",
    "colors = ['#2ecc71', '#f39c12', '#e74c3c', '#8e44ad']\n",
    "pain_labels = ['No Pain', 'Mild', 'Moderate', 'Severe']\n",
    "\n",
    "axes[0].bar(pain_counts.index, pain_counts.values, color=colors[:len(pain_counts)])\n",
    "axes[0].set_xlabel('Pain Level')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Distribution of Pain Levels')\n",
    "axes[0].set_xticks(pain_counts.index)\n",
    "axes[0].set_xticklabels(pain_labels[:len(pain_counts)])\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(pain_counts.values, labels=pain_labels[:len(pain_counts)], \n",
    "            autopct='%1.1f%%', colors=colors[:len(pain_counts)], startangle=90)\n",
    "axes[1].set_title('Pain Level Proportions')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nClass Distribution:')\n",
    "print(df[TARGET].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95894770",
   "metadata": {},
   "source": [
    "### 6.2 Numerical Features Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2d2a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# NUMERICAL FEATURES - Histograms & Boxplots\n",
    "# ============================================================\n",
    "\n",
    "num_features = [col for col in numerical_cols if col != TARGET]\n",
    "n = len(num_features)\n",
    "\n",
    "fig, axes = plt.subplots(n, 2, figsize=(14, 4 * n))\n",
    "if n == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for i, col in enumerate(num_features):\n",
    "    # Histogram\n",
    "    axes[i, 0].hist(df[col], bins=20, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "    axes[i, 0].set_title(f'{col} - Distribution')\n",
    "    axes[i, 0].set_xlabel(col)\n",
    "    axes[i, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Boxplot\n",
    "    axes[i, 1].boxplot(df[col].dropna(), vert=True)\n",
    "    axes[i, 1].set_title(f'{col} - Box Plot')\n",
    "    axes[i, 1].set_ylabel(col)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9da7197",
   "metadata": {},
   "source": [
    "### 6.3 Categorical Features Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938e77be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CATEGORICAL FEATURES - Count plots\n",
    "# ============================================================\n",
    "\n",
    "cat_features = [col for col in categorical_cols if col != TARGET]\n",
    "n_cat = len(cat_features)\n",
    "\n",
    "if n_cat > 0:\n",
    "    fig, axes = plt.subplots(1, n_cat, figsize=(6 * n_cat, 5))\n",
    "    if n_cat == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, col in enumerate(cat_features):\n",
    "        sns.countplot(data=df, x=col, ax=axes[i], palette='viridis')\n",
    "        axes[i].set_title(f'{col} Distribution')\n",
    "        axes[i].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No categorical features to plot.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f53456",
   "metadata": {},
   "source": [
    "### 6.4 Pain Levels Across Study Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2995be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PAIN LEVELS vs STUDY LOCATION\n",
    "# ============================================================\n",
    "# UPDATE column names as needed\n",
    "\n",
    "STUDY_LOCATION = 'study_location'  # <-- UPDATE THIS\n",
    "\n",
    "if STUDY_LOCATION in df.columns:\n",
    "    fig = px.histogram(df, x=STUDY_LOCATION, color=TARGET, \n",
    "                       barmode='group',\n",
    "                       title='Pain Level Distribution Across Study Locations',\n",
    "                       labels={STUDY_LOCATION: 'Study Location', TARGET: 'Pain Level'},\n",
    "                       color_discrete_sequence=px.colors.qualitative.Set2)\n",
    "    fig.show()\n",
    "    \n",
    "    # Cross-tabulation\n",
    "    print('\\nCross-tabulation: Study Location vs Pain Level')\n",
    "    print(pd.crosstab(df[STUDY_LOCATION], df[TARGET], margins=True))\n",
    "else:\n",
    "    print(f'Column \"{STUDY_LOCATION}\" not found. Update the variable name.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5892f6ec",
   "metadata": {},
   "source": [
    "### 6.5 Pain Levels Across Seat Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a57ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PAIN LEVELS vs SEAT TYPE\n",
    "# ============================================================\n",
    "\n",
    "SEAT_TYPE = 'seat_type'  # <-- UPDATE THIS\n",
    "\n",
    "if SEAT_TYPE in df.columns:\n",
    "    fig = px.histogram(df, x=SEAT_TYPE, color=TARGET,\n",
    "                       barmode='group',\n",
    "                       title='Pain Level Distribution Across Seat Types',\n",
    "                       labels={SEAT_TYPE: 'Seat Type', TARGET: 'Pain Level'},\n",
    "                       color_discrete_sequence=px.colors.qualitative.Pastel)\n",
    "    fig.show()\n",
    "else:\n",
    "    print(f'Column \"{SEAT_TYPE}\" not found. Update the variable name.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c5030e",
   "metadata": {},
   "source": [
    "### 6.6 Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d00c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CORRELATION HEATMAP (numerical features only)\n",
    "# ============================================================\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "corr_matrix = df[numerical_cols].corr()\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='RdBu_r',\n",
    "            center=0, square=True, linewidths=0.5)\n",
    "plt.title('Correlation Heatmap of Numerical Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e983ec",
   "metadata": {},
   "source": [
    "### 6.7 Pairplot of Key Features vs Pain Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625368c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PAIRPLOT - Key features vs target\n",
    "# ============================================================\n",
    "# Select the most important numerical features for pairplot\n",
    "\n",
    "# UPDATE these with your actual column names\n",
    "key_features = numerical_cols[:4]  # Take first 4 numerical features, or specify manually\n",
    "\n",
    "if len(key_features) > 1:\n",
    "    plot_df = df[key_features + [TARGET]].copy()\n",
    "    plot_df[TARGET] = plot_df[TARGET].astype(str)\n",
    "    sns.pairplot(plot_df, hue=TARGET, palette='Set2', diag_kind='kde')\n",
    "    plt.suptitle('Pairplot of Key Features by Pain Level', y=1.02)\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Not enough numerical features for pairplot.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4447ff",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Feature Selection & Importance\n",
    "\n",
    "Identify which study habits and environmental factors contribute most to physical pain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045c7606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FEATURE IMPORTANCE using Random Forest\n",
    "# ============================================================\n",
    "\n",
    "# Prepare data for feature importance analysis\n",
    "df_encoded = df.copy()\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    if col in df_encoded.columns:\n",
    "        le = LabelEncoder()\n",
    "        df_encoded[col] = le.fit_transform(df_encoded[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "\n",
    "# Split features and target\n",
    "X_imp = df_encoded.drop(columns=[TARGET])\n",
    "y_imp = df_encoded[TARGET]\n",
    "\n",
    "# Train a Random Forest for feature importance\n",
    "rf_importance = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_importance.fit(X_imp, y_imp)\n",
    "\n",
    "# Get feature importances\n",
    "importances = pd.DataFrame({\n",
    "    'Feature': X_imp.columns,\n",
    "    'Importance': rf_importance.feature_importances_\n",
    "}).sort_values('Importance', ascending=True)\n",
    "\n",
    "# Plot\n",
    "fig = px.bar(importances, x='Importance', y='Feature', orientation='h',\n",
    "             title='Feature Importance (Random Forest)',\n",
    "             color='Importance', color_continuous_scale='RdYlGn_r')\n",
    "fig.update_layout(height=500)\n",
    "fig.show()\n",
    "\n",
    "print('\\nTop Risk Factors (Features most correlated with pain):')\n",
    "print(importances.tail(5).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e81cb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PERMUTATION IMPORTANCE (more robust measure)\n",
    "# ============================================================\n",
    "\n",
    "X_train_imp, X_test_imp, y_train_imp, y_test_imp = train_test_split(\n",
    "    X_imp, y_imp, test_size=0.2, random_state=42, stratify=y_imp\n",
    ")\n",
    "\n",
    "rf_perm = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_perm.fit(X_train_imp, y_train_imp)\n",
    "\n",
    "perm_imp = permutation_importance(rf_perm, X_test_imp, y_test_imp, \n",
    "                                   n_repeats=10, random_state=42)\n",
    "\n",
    "perm_df = pd.DataFrame({\n",
    "    'Feature': X_imp.columns,\n",
    "    'Importance Mean': perm_imp.importances_mean,\n",
    "    'Importance Std': perm_imp.importances_std\n",
    "}).sort_values('Importance Mean', ascending=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.barh(perm_df['Feature'], perm_df['Importance Mean'], \n",
    "        xerr=perm_df['Importance Std'], color='coral')\n",
    "ax.set_title('Permutation Feature Importance')\n",
    "ax.set_xlabel('Mean Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b499ad3d",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Data Preprocessing for Modeling\n",
    "\n",
    "Encode categorical features, scale numerical features, and split data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e719ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PREPARE DATA FOR MODELING\n",
    "# ============================================================\n",
    "\n",
    "# Use the encoded dataframe\n",
    "X = df_encoded.drop(columns=[TARGET])\n",
    "y = df_encoded[TARGET]\n",
    "\n",
    "print(f'Features shape: {X.shape}')\n",
    "print(f'Target shape: {y.shape}')\n",
    "print(f'\\nTarget distribution:')\n",
    "print(y.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252ea10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAIN-TEST SPLIT (Stratified)\n",
    "# ============================================================\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f'Training set: {X_train.shape[0]} samples')\n",
    "print(f'Test set: {X_test.shape[0]} samples')\n",
    "print(f'\\nTraining target distribution:')\n",
    "print(y_train.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a85c1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FEATURE SCALING\n",
    "# ============================================================\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrame for readability\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print('Feature scaling complete.')\n",
    "print(f'\\nScaled training data sample:')\n",
    "X_train_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882daad9",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Model Development & Comparison\n",
    "\n",
    "Train and compare multiple classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b923f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DEFINE MODELS\n",
    "# ============================================================\n",
    "\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(kernel='rbf', random_state=42),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "}\n",
    "\n",
    "print(f'Models to train: {list(models.keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a73481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAIN & EVALUATE ALL MODELS\n",
    "# ============================================================\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'Training: {name}')\n",
    "    print(f'{\"=\"*60}')\n",
    "    \n",
    "    # Train\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1_weighted = f1_score(y_test, y_pred, average='weighted')\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': acc,\n",
    "        'F1 (Weighted)': f1_weighted,\n",
    "        'F1 (Macro)': f1_macro,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall\n",
    "    })\n",
    "    \n",
    "    print(f'Accuracy: {acc:.4f}')\n",
    "    print(f'F1 (Weighted): {f1_weighted:.4f}')\n",
    "    print(f'F1 (Macro): {f1_macro:.4f}')\n",
    "    print(f'\\nClassification Report:')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Summary table\n",
    "results_df = pd.DataFrame(results).sort_values('F1 (Weighted)', ascending=False)\n",
    "print('\\n' + '='*60)\n",
    "print('MODEL COMPARISON SUMMARY')\n",
    "print('='*60)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f2596c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUAL MODEL COMPARISON\n",
    "# ============================================================\n",
    "\n",
    "fig = px.bar(results_df, x='Model', y=['Accuracy', 'F1 (Weighted)', 'F1 (Macro)', 'Precision', 'Recall'],\n",
    "             barmode='group', title='Model Performance Comparison',\n",
    "             color_discrete_sequence=px.colors.qualitative.Set2)\n",
    "fig.update_layout(yaxis_title='Score', xaxis_title='Model', legend_title='Metric')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e96a0c3",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Model Evaluation & Validation\n",
    "\n",
    "### 10.1 Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dabc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFUSION MATRICES for all models\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (name, model) in enumerate(models.items()):\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(cm)\n",
    "    disp.plot(ax=axes[i], cmap='Blues', colorbar=False)\n",
    "    axes[i].set_title(f'{name}')\n",
    "\n",
    "plt.suptitle('Confusion Matrices - All Models', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37eb8f90",
   "metadata": {},
   "source": [
    "### 10.2 Stratified K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9fc9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STRATIFIED K-FOLD CROSS-VALIDATION\n",
    "# ============================================================\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_results = []\n",
    "\n",
    "# Scale all data for CV\n",
    "X_all_scaled = scaler.fit_transform(X)\n",
    "\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X_all_scaled, y, cv=skf, scoring='f1_weighted')\n",
    "    cv_results.append({\n",
    "        'Model': name,\n",
    "        'CV Mean F1': scores.mean(),\n",
    "        'CV Std F1': scores.std(),\n",
    "        'CV Scores': scores\n",
    "    })\n",
    "    print(f'{name}: F1 = {scores.mean():.4f} (+/- {scores.std():.4f})')\n",
    "\n",
    "cv_df = pd.DataFrame(cv_results)[['Model', 'CV Mean F1', 'CV Std F1']].sort_values('CV Mean F1', ascending=False)\n",
    "print('\\n' + '='*60)\n",
    "print('CROSS-VALIDATION RESULTS')\n",
    "print('='*60)\n",
    "cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028a5be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CV RESULTS BOXPLOT\n",
    "# ============================================================\n",
    "\n",
    "cv_scores_dict = {r['Model']: r['CV Scores'] for r in cv_results}\n",
    "cv_box_df = pd.DataFrame(cv_scores_dict)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "cv_box_df.boxplot(ax=ax)\n",
    "ax.set_title('Stratified 5-Fold Cross-Validation F1 Scores')\n",
    "ax.set_ylabel('F1 Score (Weighted)')\n",
    "ax.set_xlabel('Model')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fd9c4e",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Final Model Selection & Explainability\n",
    "\n",
    "Select the best model and make its decision logic explainable (White-box AI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6330d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SELECT BEST MODEL\n",
    "# ============================================================\n",
    "\n",
    "best_model_name = cv_df.iloc[0]['Model']\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "# Retrain on full training data\n",
    "best_model.fit(X_train_scaled, y_train)\n",
    "y_pred_best = best_model.predict(X_test_scaled)\n",
    "\n",
    "print(f'Best Model: {best_model_name}')\n",
    "print(f'\\nFinal Classification Report:')\n",
    "print(classification_report(y_test, y_pred_best))\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=pain_labels[:len(np.unique(y))],\n",
    "            yticklabels=pain_labels[:len(np.unique(y))])\n",
    "plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f9e77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MODEL EXPLAINABILITY - Decision Tree Visualization\n",
    "# ============================================================\n",
    "# Decision Trees are inherently interpretable (White-box)\n",
    "\n",
    "from sklearn.tree import export_text, plot_tree\n",
    "\n",
    "# Train a simple Decision Tree for explainability\n",
    "dt_explainer = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
    "dt_explainer.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Text representation\n",
    "tree_rules = export_text(dt_explainer, feature_names=list(X.columns))\n",
    "print('Decision Tree Rules (max_depth=4):')\n",
    "print(tree_rules)\n",
    "\n",
    "# Visual representation\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(dt_explainer, feature_names=list(X.columns), \n",
    "          class_names=pain_labels[:len(np.unique(y))],\n",
    "          filled=True, rounded=True, fontsize=8)\n",
    "plt.title('Decision Tree Visualization (Explainable Model)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e069e98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# KEY RISK FACTORS SUMMARY\n",
    "# ============================================================\n",
    "\n",
    "print('='*60)\n",
    "print('TOP 3 RISK FACTORS (Actionable Insights)')\n",
    "print('='*60)\n",
    "\n",
    "top_features = importances.tail(3)\n",
    "for i, (_, row) in enumerate(top_features.iterrows(), 1):\n",
    "    print(f'\\n{i}. {row[\"Feature\"]} (Importance: {row[\"Importance\"]:.4f})')\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('RECOMMENDATIONS')\n",
    "print('='*60)\n",
    "print('Based on the analysis, the following changes can reduce pain risk:')\n",
    "print('(These will be filled based on actual feature importance results)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80c2c07",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Save Model & Demo\n",
    "\n",
    "Save the trained model and create an interactive demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49dc7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SAVE MODEL & SCALER\n",
    "# ============================================================\n",
    "\n",
    "joblib.dump(best_model, '../models/somatrack_model.joblib')\n",
    "joblib.dump(scaler, '../models/somatrack_scaler.joblib')\n",
    "joblib.dump(label_encoders, '../models/somatrack_encoders.joblib')\n",
    "\n",
    "print(f'Model saved: somatrack_model.joblib')\n",
    "print(f'Scaler saved: somatrack_scaler.joblib')\n",
    "print(f'Encoders saved: somatrack_encoders.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d49e1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# INTERACTIVE DEMO - Ergonomic Risk Audit\n",
    "# ============================================================\n",
    "\n",
    "def predict_pain_risk(input_data: dict):\n",
    "    \"\"\"\n",
    "    Predict pain risk for a student based on their study habits.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_data : dict\n",
    "        Dictionary with feature names as keys and values.\n",
    "        Example: {\n",
    "            'sitting_hours_per_day': 6,\n",
    "            'consecutive_hours_no_break': 3,\n",
    "            'water_liters_per_day': 1.5,\n",
    "            'daily_screen_time_hours': 8,\n",
    "            'study_location': 'Bed',\n",
    "            'seat_type': 'No chair (bed)',\n",
    "            'uses_peripherals': 'No',\n",
    "            ...\n",
    "        }\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Prediction and risk assessment.\n",
    "    \"\"\"\n",
    "    # Load model, scaler, encoders\n",
    "    model = joblib.load('../models/somatrack_model.joblib')\n",
    "    scaler = joblib.load('../models/somatrack_scaler.joblib')\n",
    "    encoders = joblib.load('../models/somatrack_encoders.joblib')\n",
    "    \n",
    "    # Create DataFrame\n",
    "    input_df = pd.DataFrame([input_data])\n",
    "    \n",
    "    # Encode categorical features\n",
    "    for col, le in encoders.items():\n",
    "        if col in input_df.columns:\n",
    "            input_df[col] = le.transform(input_df[col].astype(str))\n",
    "    \n",
    "    # Scale\n",
    "    input_scaled = scaler.transform(input_df)\n",
    "    \n",
    "    # Predict\n",
    "    prediction = model.predict(input_scaled)[0]\n",
    "    \n",
    "    risk_labels = {\n",
    "        0: 'No Pain - Low Risk',\n",
    "        1: 'Mild Pain - Moderate Risk',\n",
    "        2: 'Frequent Pain - High Risk',\n",
    "        3: 'Chronic Pain - Very High Risk'\n",
    "    }\n",
    "    \n",
    "    print('='*50)\n",
    "    print('SOMATRACK - ERGONOMIC RISK AUDIT')\n",
    "    print('='*50)\n",
    "    print(f'\\nInput: {input_data}')\n",
    "    print(f'\\nPrediction: {risk_labels.get(prediction, \"Unknown\")}')\n",
    "    print(f'Pain Level: {prediction}')\n",
    "    print('='*50)\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "print('Demo function ready. Use predict_pain_risk() with a dictionary of study habits.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220adfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DEMO EXAMPLE\n",
    "# ============================================================\n",
    "# UPDATE feature names and values based on your actual data\n",
    "\n",
    "# Example: A student who studies in bed for 6 hours with no mouse\n",
    "sample_student = {\n",
    "    # UPDATE these keys to match your actual column names\n",
    "    # 'sitting_hours_per_day': 6,\n",
    "    # 'consecutive_hours_no_break': 3,\n",
    "    # 'water_liters_per_day': 1.0,\n",
    "    # 'daily_screen_time_hours': 8,\n",
    "    # 'study_location': 'Bed',\n",
    "    # 'seat_type': 'No seat (bed/couch)',\n",
    "    # 'uses_peripherals': 'No',\n",
    "    # 'exercise_frequency': 0,\n",
    "    # 'takes_breaks': 'No',\n",
    "    # 'posture_rating': 3,\n",
    "}\n",
    "\n",
    "# Uncomment the line below once sample_student is filled:\n",
    "# predict_pain_risk(sample_student)\n",
    "\n",
    "print('Update the sample_student dictionary with your actual feature names and run the demo.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3ffeb0",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Conclusions & Recommendations\n",
    "\n",
    "### Key Findings\n",
    "- *TODO: Summarize the top risk factors identified*\n",
    "- *TODO: Which model performed best and why*\n",
    "- *TODO: Distribution of pain levels across the student population*\n",
    "\n",
    "### Actionable Recommendations\n",
    "1. **Risk Factor 1**: *TODO: e.g., \"Students studying in bed are X% more likely to experience neck pain\"*\n",
    "2. **Risk Factor 2**: *TODO: e.g., \"Taking breaks every 2 hours reduces pain risk by Y%\"*\n",
    "3. **Risk Factor 3**: *TODO: e.g., \"Using an ergonomic chair reduces back pain by Z%\"*\n",
    "\n",
    "### Model Performance\n",
    "- Best Model: *TODO*\n",
    "- F1 Score (Weighted): *TODO*\n",
    "- The model is explainable (White-box AI) through Decision Tree visualization\n",
    "\n",
    "### Limitations\n",
    "- Self-reported data may have bias\n",
    "- Sample size considerations\n",
    "- Cross-sectional study (not longitudinal)\n",
    "\n",
    "---\n",
    "*SomaTrack - Predicting Study-Related Physical Ailments*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
